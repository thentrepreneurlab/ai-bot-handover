{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7fa1a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137fc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5be9a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8770c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f145b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_chat_agent(name, description, prompt_fn, tools=None):\n",
    "    async def run(inputs):\n",
    "        prompt = await prompt_fn(inputs)\n",
    "        # run LLM or toolchain\n",
    "        response = await call_model(prompt, tools)\n",
    "        return response\n",
    "\n",
    "    return AgentNode(\n",
    "        name=name,\n",
    "        description=description,\n",
    "        run=run,\n",
    "        tools=tools or []\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e84393b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c18b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90b81b3a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c8b3b5f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "184df87a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50ce2e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_chat_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33m    You are a professional logo designer AI.\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[33m    Generate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m clean, vector-style logo concepts.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m \u001b[33m    \u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# --- Agent definition ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m LogoGeneratorAgent = \u001b[43minit_chat_agent\u001b[49m(\n\u001b[32m     46\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mLogoGeneratorAgent\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     47\u001b[39m     description=\u001b[33m\"\u001b[39m\u001b[33mGenerates logo concepts based on brand name, style, and industry.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     48\u001b[39m     prompt_fn=logo_agent_prompt,\n\u001b[32m     49\u001b[39m     tools=[image_gen],\n\u001b[32m     50\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'init_chat_agent' is not defined"
     ]
    }
   ],
   "source": [
    "# # agents/logo_agent.py\n",
    "# from agents import init_chat_agent\n",
    "# from tools import image_gen  # ya jo bhi tu ka call kare image generation ke liye\n",
    "\n",
    "# --- Prompt builder ---\n",
    "async def logo_agent_prompt(inputs):\n",
    "    brand = inputs.get(\"brand_name\", \"Unnamed Brand\")\n",
    "    industry = inputs.get(\"industry\", \"General\")\n",
    "    style = inputs.get(\"style\", \"modern minimalist\")\n",
    "    colors = inputs.get(\"colors\", \"blue, white\")\n",
    "    tagline = inputs.get(\"tagline\", \"\")\n",
    "    count = inputs.get(\"count\", 4)\n",
    "\n",
    "    return f\"\"\"\n",
    "    You are a professional logo designer AI.\n",
    "    Generate {count} clean, vector-style logo concepts.\n",
    "\n",
    "    Brand: {brand}\n",
    "    Tagline: {tagline}\n",
    "    Industry: {industry}\n",
    "    Style: {style}\n",
    "    Colors: {colors}\n",
    "\n",
    "    Constraints:\n",
    "    - Centered composition\n",
    "    - Transparent background\n",
    "    - Flat shapes, simple geometry\n",
    "    - Scalable and clear at 48x48 and 1024x1024\n",
    "    - No photorealism or text-only logos unless specified\n",
    "\n",
    "    Return results as JSON with key 'image_response' containing:\n",
    "    {{\n",
    "        \"type\": \"image_response\",\n",
    "        \"count\": <number>,\n",
    "        \"logos\": [\n",
    "            {{\n",
    "                \"url\": \"<image-url>\",\n",
    "                \"description\": \"<short visual summary>\"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "LogoGeneratorAgent = init_chat_model(\n",
    "    \n",
    "    name=\"LogoGeneratorAgent\",\n",
    "    description=\"Generates logo concepts based on brand name, style, and industry.\",\n",
    "    prompt_fn=logo_agent_prompt,\n",
    "    tools=[image_gen],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "002e5808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function init_chat_model in module langchain.chat_models.base:\n",
      "\n",
      "init_chat_model(model: 'Optional[str]' = None, *, model_provider: 'Optional[str]' = None, configurable_fields: \"Optional[Union[Literal['any'], list[str], tuple[str, ...]]]\" = None, config_prefix: 'Optional[str]' = None, **kwargs: 'Any') -> 'Union[BaseChatModel, _ConfigurableModel]'\n",
      "    Initialize a ChatModel in a single line using the model's name and provider.\n",
      "\n",
      "    .. note::\n",
      "        Must have the integration package corresponding to the model provider installed.\n",
      "        You should look at the `provider integration's API reference <https://python.langchain.com/api_reference/reference.html#integrations>`__\n",
      "        to see what parameters are supported by the model.\n",
      "\n",
      "    Args:\n",
      "        model: The name of the model, e.g. ``'o3-mini'``, ``'claude-3-5-sonnet-latest'``. You can\n",
      "            also specify model and model provider in a single argument using\n",
      "            ``'{model_provider}:{model}'`` format, e.g. ``'openai:o1'``.\n",
      "        model_provider: The model provider if not specified as part of model arg (see\n",
      "            above). Supported model_provider values and the corresponding integration\n",
      "            package are:\n",
      "\n",
      "            - ``openai``              -> ``langchain-openai``\n",
      "            - ``anthropic``           -> ``langchain-anthropic``\n",
      "            - ``azure_openai``        -> ``langchain-openai``\n",
      "            - ``azure_ai``            -> ``langchain-azure-ai``\n",
      "            - ``google_vertexai``     -> ``langchain-google-vertexai``\n",
      "            - ``google_genai``        -> ``langchain-google-genai``\n",
      "            - ``bedrock``             -> ``langchain-aws``\n",
      "            - ``bedrock_converse``    -> ``langchain-aws``\n",
      "            - ``cohere``              -> ``langchain-cohere``\n",
      "            - ``fireworks``           -> ``langchain-fireworks``\n",
      "            - ``together``            -> ``langchain-together``\n",
      "            - ``mistralai``           -> ``langchain-mistralai``\n",
      "            - ``huggingface``         -> ``langchain-huggingface``\n",
      "            - ``groq``                -> ``langchain-groq``\n",
      "            - ``ollama``              -> ``langchain-ollama``\n",
      "            - ``google_anthropic_vertex``    -> ``langchain-google-vertexai``\n",
      "            - ``deepseek``            -> ``langchain-deepseek``\n",
      "            - ``ibm``                 -> ``langchain-ibm``\n",
      "            - ``nvidia``              -> ``langchain-nvidia-ai-endpoints``\n",
      "            - ``xai``                 -> ``langchain-xai``\n",
      "            - ``perplexity``          -> ``langchain-perplexity``\n",
      "\n",
      "            Will attempt to infer model_provider from model if not specified. The\n",
      "            following providers will be inferred based on these model prefixes:\n",
      "\n",
      "            - ``gpt-3...`` | ``gpt-4...`` | ``o1...`` -> ``openai``\n",
      "            - ``claude...``                       -> ``anthropic``\n",
      "            - ``amazon...``                       -> ``bedrock``\n",
      "            - ``gemini...``                       -> ``google_vertexai``\n",
      "            - ``command...``                      -> ``cohere``\n",
      "            - ``accounts/fireworks...``           -> ``fireworks``\n",
      "            - ``mistral...``                      -> ``mistralai``\n",
      "            - ``deepseek...``                     -> ``deepseek``\n",
      "            - ``grok...``                         -> ``xai``\n",
      "            - ``sonar...``                        -> ``perplexity``\n",
      "        configurable_fields: Which model parameters are configurable:\n",
      "\n",
      "            - None: No configurable fields.\n",
      "            - ``'any'``: All fields are configurable. **See Security Note below.**\n",
      "            - Union[List[str], Tuple[str, ...]]: Specified fields are configurable.\n",
      "\n",
      "            Fields are assumed to have config_prefix stripped if there is a\n",
      "            config_prefix. If model is specified, then defaults to None. If model is\n",
      "            not specified, then defaults to ``(\"model\", \"model_provider\")``.\n",
      "\n",
      "            ***Security Note***: Setting ``configurable_fields=\"any\"`` means fields like\n",
      "            ``api_key``, ``base_url``, etc. can be altered at runtime, potentially redirecting\n",
      "            model requests to a different service/user. Make sure that if you're\n",
      "            accepting untrusted configurations that you enumerate the\n",
      "            ``configurable_fields=(...)`` explicitly.\n",
      "\n",
      "        config_prefix: If ``'config_prefix'`` is a non-empty string then model will be\n",
      "            configurable at runtime via the\n",
      "            ``config[\"configurable\"][\"{config_prefix}_{param}\"]`` keys. If\n",
      "            ``'config_prefix'`` is an empty string then model will be configurable via\n",
      "            ``config[\"configurable\"][\"{param}\"]``.\n",
      "        temperature: Model temperature.\n",
      "        max_tokens: Max output tokens.\n",
      "        timeout: The maximum time (in seconds) to wait for a response from the model\n",
      "            before canceling the request.\n",
      "        max_retries: The maximum number of attempts the system will make to resend a\n",
      "            request if it fails due to issues like network timeouts or rate limits.\n",
      "        base_url: The URL of the API endpoint where requests are sent.\n",
      "        rate_limiter: A ``BaseRateLimiter`` to space out requests to avoid exceeding\n",
      "            rate limits.\n",
      "        kwargs: Additional model-specific keyword args to pass to\n",
      "            ``<<selected ChatModel>>.__init__(model=model_name, **kwargs)``.\n",
      "\n",
      "    Returns:\n",
      "        A BaseChatModel corresponding to the model_name and model_provider specified if\n",
      "        configurability is inferred to be False. If configurable, a chat model emulator\n",
      "        that initializes the underlying model at runtime once a config is passed in.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If model_provider cannot be inferred or isn't supported.\n",
      "        ImportError: If the model provider integration package is not installed.\n",
      "\n",
      "    .. dropdown:: Init non-configurable model\n",
      "        :open:\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            # pip install langchain langchain-openai langchain-anthropic langchain-google-vertexai\n",
      "            from langchain.chat_models import init_chat_model\n",
      "\n",
      "            o3_mini = init_chat_model(\"openai:o3-mini\", temperature=0)\n",
      "            claude_sonnet = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\", temperature=0)\n",
      "            gemini_2_flash = init_chat_model(\"google_vertexai:gemini-2.0-flash\", temperature=0)\n",
      "\n",
      "            o3_mini.invoke(\"what's your name\")\n",
      "            claude_sonnet.invoke(\"what's your name\")\n",
      "            gemini_2_flash.invoke(\"what's your name\")\n",
      "\n",
      "\n",
      "    .. dropdown:: Partially configurable model with no default\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            # pip install langchain langchain-openai langchain-anthropic\n",
      "            from langchain.chat_models import init_chat_model\n",
      "\n",
      "            # We don't need to specify configurable=True if a model isn't specified.\n",
      "            configurable_model = init_chat_model(temperature=0)\n",
      "\n",
      "            configurable_model.invoke(\n",
      "                \"what's your name\",\n",
      "                config={\"configurable\": {\"model\": \"gpt-4o\"}}\n",
      "            )\n",
      "            # GPT-4o response\n",
      "\n",
      "            configurable_model.invoke(\n",
      "                \"what's your name\",\n",
      "                config={\"configurable\": {\"model\": \"claude-3-5-sonnet-latest\"}}\n",
      "            )\n",
      "            # claude-3.5 sonnet response\n",
      "\n",
      "    .. dropdown:: Fully configurable model with a default\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            # pip install langchain langchain-openai langchain-anthropic\n",
      "            from langchain.chat_models import init_chat_model\n",
      "\n",
      "            configurable_model_with_default = init_chat_model(\n",
      "                \"openai:gpt-4o\",\n",
      "                configurable_fields=\"any\",  # this allows us to configure other params like temperature, max_tokens, etc at runtime.\n",
      "                config_prefix=\"foo\",\n",
      "                temperature=0\n",
      "            )\n",
      "\n",
      "            configurable_model_with_default.invoke(\"what's your name\")\n",
      "            # GPT-4o response with temperature 0\n",
      "\n",
      "            configurable_model_with_default.invoke(\n",
      "                \"what's your name\",\n",
      "                config={\n",
      "                    \"configurable\": {\n",
      "                        \"foo_model\": \"anthropic:claude-3-5-sonnet-20240620\",\n",
      "                        \"foo_temperature\": 0.6\n",
      "                    }\n",
      "                }\n",
      "            )\n",
      "            # Claude-3.5 sonnet response with temperature 0.6\n",
      "\n",
      "    .. dropdown:: Bind tools to a configurable model\n",
      "\n",
      "        You can call any ChatModel declarative methods on a configurable model in the\n",
      "        same way that you would with a normal model.\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            # pip install langchain langchain-openai langchain-anthropic\n",
      "            from langchain.chat_models import init_chat_model\n",
      "            from pydantic import BaseModel, Field\n",
      "\n",
      "            class GetWeather(BaseModel):\n",
      "                '''Get the current weather in a given location'''\n",
      "\n",
      "                location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
      "\n",
      "            class GetPopulation(BaseModel):\n",
      "                '''Get the current population in a given location'''\n",
      "\n",
      "                location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
      "\n",
      "            configurable_model = init_chat_model(\n",
      "                \"gpt-4o\",\n",
      "                configurable_fields=(\"model\", \"model_provider\"),\n",
      "                temperature=0\n",
      "            )\n",
      "\n",
      "            configurable_model_with_tools = configurable_model.bind_tools([GetWeather, GetPopulation])\n",
      "            configurable_model_with_tools.invoke(\n",
      "                \"Which city is hotter today and which is bigger: LA or NY?\"\n",
      "            )\n",
      "            # GPT-4o response with tool calls\n",
      "\n",
      "            configurable_model_with_tools.invoke(\n",
      "                \"Which city is hotter today and which is bigger: LA or NY?\",\n",
      "                config={\"configurable\": {\"model\": \"claude-3-5-sonnet-20240620\"}}\n",
      "            )\n",
      "            # Claude-3.5 sonnet response with tools\n",
      "\n",
      "    .. versionadded:: 0.2.7\n",
      "\n",
      "    .. versionchanged:: 0.2.8\n",
      "\n",
      "        Support for ``configurable_fields`` and ``config_prefix`` added.\n",
      "\n",
      "    .. versionchanged:: 0.2.12\n",
      "\n",
      "        Support for Ollama via langchain-ollama package added\n",
      "        (langchain_ollama.ChatOllama). Previously,\n",
      "        the now-deprecated langchain-community version of Ollama was imported\n",
      "        (langchain_community.chat_models.ChatOllama).\n",
      "\n",
      "        Support for AWS Bedrock models via the Converse API added\n",
      "        (model_provider=\"bedrock_converse\").\n",
      "\n",
      "    .. versionchanged:: 0.3.5\n",
      "\n",
      "        Out of beta.\n",
      "\n",
      "    .. versionchanged:: 0.3.19\n",
      "\n",
      "        Support for Deepseek, IBM, Nvidia, and xAI models added.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(init_chat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ee0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f48e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa005466",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1a10e0a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbe8b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4531c693",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogoGeneratorAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mLogoGeneratorAgent\u001b[49m.run({\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbrand_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mNovaAI\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mindustry\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mAI SaaS\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mstyle\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmodern minimalist\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolors\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mblue, white\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m })\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[31mNameError\u001b[39m: name 'LogoGeneratorAgent' is not defined"
     ]
    }
   ],
   "source": [
    "response = await LogoGeneratorAgent.run({\n",
    "    \"brand_name\": \"NovaAI\",\n",
    "    \"industry\": \"AI SaaS\",\n",
    "    \"style\": \"modern minimalist\",\n",
    "    \"colors\": \"blue, white\"\n",
    "})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e60b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ded4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brunda-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
